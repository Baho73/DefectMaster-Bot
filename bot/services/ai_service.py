"""
AI Service for analyzing construction photos using Google Gemini
"""
import json
import google.generativeai as genai
from typing import Dict, Any, Optional
from PIL import Image
import io
import logging
import config
from bot.services.settings_service import settings_service
from bot.utils.markdown_utils import escape_markdown

logger = logging.getLogger(__name__)


class AIService:
    """Service for analyzing construction defects using Google Gemini"""

    SYSTEM_PROMPT = """–†–û–õ–¨:
–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π –∏ –æ–ø—ã—Ç–Ω—ã–π –∏–Ω–∂–µ–Ω–µ—Ä —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è (–¢–µ—Ö–Ω–∞–¥–∑–æ—Ä) –≤ –†–§. –¢–≤–æ—è —Ü–µ–ª—å ‚Äî –Ω–∞–π—Ç–∏ –Ω–∞—Ä—É—à–µ–Ω–∏—è, –æ—Ü–µ–Ω–∏—Ç—å –∏—Ö –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å –∏ —Å–æ—Å–ª–∞—Ç—å—Å—è –Ω–∞ –Ω–æ—Ä–º—ã.

–ó–ê–î–ê–ß–ê:
1. –ü—Ä–æ–≤–µ—Ä—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–æ—Ç–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–º. –ï—Å–ª–∏ —ç—Ç–æ –∫–æ—Ç, –µ–¥–∞, —Å–µ–ª—Ñ–∏ –∏–ª–∏ —è–≤–Ω—ã–π –º—É—Å–æ—Ä ‚Äî –≤–µ—Ä–Ω–∏ "is_relevant": false –∏ —à—É—Ç–ª–∏–≤—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π.
2. –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–π–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (–û–±—ä–µ–∫—Ç/–ú–µ—Å—Ç–æ).
3. –ù–∞–π–¥–∏ –¥–µ—Ñ–µ–∫—Ç—ã. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ñ–µ–∫—Ç–∞ –æ–ø—Ä–µ–¥–µ–ª–∏:
   - –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ.
   - –¢–æ—á–Ω–æ–µ –º–µ—Å—Ç–æ–Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –Ω–∞ —Ñ–æ—Ç–æ.
   - –ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å (–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π / –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π / –ú–∞–ª–æ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π).
   - –í–µ—Ä–æ—è—Ç–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞.
   - –ù–∞—Ä—É—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞ –†–§ (–°–ü, –ì–û–°–¢, –°–ù–∏–ü, –ü—Ä–∏–∫–∞–∑ –ú–∏–Ω—Ç—Ä—É–¥–∞). –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∂–∏ –Ω–æ–º–µ—Ä –ø—É–Ω–∫—Ç–∞.
   - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é (–∏–º–ø–µ—Ä–∞—Ç–∏–≤: "–°–¥–µ–ª–∞—Ç—å", "–£—Å—Ç—Ä–∞–Ω–∏—Ç—å").
4. –°—Ñ–æ—Ä–º–∏—Ä—É–π –∫—Ä–∞—Ç–∫–æ–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –ø–æ —Ñ–æ—Ç–æ.

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{
  "is_relevant": true,
  "joke": null,
  "items": [
    {
      "defect": "–ù–∞–∑–≤–∞–Ω–∏–µ –¥–µ—Ñ–µ–∫—Ç–∞",
      "location": "–ì–¥–µ –∏–º–µ–Ω–Ω–æ –Ω–∞ —Ñ–æ—Ç–æ",
      "criticality": "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π",
      "cause": "–ü—Ä–∏—á–∏–Ω–∞",
      "norm": "–°–ü 70.13330 –ø. 5.17.4",
      "recommendation": "–¢–µ–∫—Å—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"
    }
  ],
  "expert_summary": "–¢–µ–∫—Å—Ç –∑–∞–∫–ª—é—á–µ–Ω–∏—è (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)."
}

–ï—Å–ª–∏ —Ñ–æ—Ç–æ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ:
{
  "is_relevant": false,
  "joke": "–ö—Ä–∞—Å–∏–≤—ã–π –∫–æ—Ç, –Ω–æ —ç—Ç–æ –Ω–µ —Å—Ç—Ä–æ–π–ø–ª–æ—â–∞–¥–∫–∞! –ü—Ä–∏—Å—ã–ª–∞–π —Ñ–æ—Ç–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞–±–æ—Ç.",
  "items": [],
  "expert_summary": null
}

–í–ê–ñ–ù–û: –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON. –ù–∏–∫–∞–∫–æ–≥–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""

    def __init__(self):
        genai.configure(api_key=config.GOOGLE_API_KEY)

        # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        self.fast_model = genai.GenerativeModel(
            config.GEMINI_FAST_MODEL,
            generation_config={
                "temperature": 0.4,
                "response_mime_type": "application/json"
            }
        )

        # –¢–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤
        self.analysis_model = genai.GenerativeModel(
            config.GEMINI_ANALYSIS_MODEL,
            generation_config={
                "temperature": 0.4,
                "response_mime_type": "application/json"
            }
        )

        logger.info(f"AI Service initialized. Fast model: {config.GEMINI_FAST_MODEL}, Analysis model: {config.GEMINI_ANALYSIS_MODEL}")

    async def check_relevance(self, photo_bytes: bytes, context: str = None) -> Dict[str, Any]:
        """
        Quick relevance check using Flash model (1-2 seconds)

        Args:
            photo_bytes: Photo binary data
            context: User-provided context

        Returns:
            Dictionary with is_relevant and joke (if not relevant)
        """
        try:
            logger.info(f"Quick relevance check. Context: {context}, Photo size: {len(photo_bytes)} bytes")

            # Load image
            image = Image.open(io.BytesIO(photo_bytes))
            logger.info(f"Image loaded. Size: {image.size}, Format: {image.format}")

            # Read settings
            settings = None
            if config.GOOGLE_SETTINGS_DOC_ID:
                try:
                    settings = settings_service.parse_ai_settings(config.GOOGLE_SETTINGS_DOC_ID)
                except Exception as e:
                    logger.warning(f"Failed to read settings, using defaults: {e}")

            # Prepare model and prompt
            if settings:
                relevance_model_name = settings['relevance_model']
                relevance_model = genai.GenerativeModel(
                    relevance_model_name,
                    generation_config={"temperature": 0.4, "response_mime_type": "application/json"}
                )
                system_prompt = settings['relevance_prompt'] if settings['relevance_prompt'] else self.SYSTEM_PROMPT
            else:
                relevance_model = self.fast_model
                relevance_model_name = config.GEMINI_FAST_MODEL
                system_prompt = self.SYSTEM_PROMPT

            logger.info(f"Checking relevance with {relevance_model_name}...")
            relevance_prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–ü—Ä–æ–≤–µ—Ä—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Ñ–æ—Ç–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–º –æ–±—ä–µ–∫—Ç–æ–º. –ï—Å–ª–∏ —ç—Ç–æ –∫–æ—Ç, –µ–¥–∞, —Å–µ–ª—Ñ–∏ –∏–ª–∏ –Ω–µ —Å—Ç—Ä–æ–π–∫–∞ - –≤–µ—Ä–Ω–∏ is_relevant: false —Å —à—É—Ç–∫–æ–π. –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–π–∫–∞ - –≤–µ—Ä–Ω–∏ is_relevant: true." if context else "–ü—Ä–æ–≤–µ—Ä—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Ñ–æ—Ç–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–º –æ–±—ä–µ–∫—Ç–æ–º."

            response = relevance_model.generate_content(
                [system_prompt, relevance_prompt, image],
                request_options={"timeout": 120}
            )

            logger.info(f"Relevance check complete")
            result = json.loads(response.text)

            return result

        except Exception as e:
            logger.error(f"Relevance check error: {str(e)}", exc_info=True)
            return {
                "is_relevant": False,
                "joke": "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–æ—Ç–æ. –ü–æ–ø—Ä–æ–±—É–π –µ—â–µ —Ä–∞–∑.",
                "error": str(e)
            }

    async def analyze_defects(self, photo_bytes: bytes, context: str = None) -> Dict[str, Any]:
        """
        Detailed defect analysis using Pro model (10-30 seconds)

        Args:
            photo_bytes: Photo binary data
            context: User-provided context

        Returns:
            Dictionary with detailed analysis results
        """
        try:
            logger.info(f"Detailed defect analysis. Context: {context}")

            # Load image
            image = Image.open(io.BytesIO(photo_bytes))

            # Read settings
            settings = None
            if config.GOOGLE_SETTINGS_DOC_ID:
                try:
                    settings = settings_service.parse_ai_settings(config.GOOGLE_SETTINGS_DOC_ID)
                except Exception as e:
                    logger.warning(f"Failed to read settings, using defaults: {e}")

            # Prepare model and prompt
            if settings:
                analysis_model_name = settings['analysis_model']
                analysis_model = genai.GenerativeModel(
                    analysis_model_name,
                    generation_config={"temperature": 0.4, "response_mime_type": "application/json"}
                )
                system_prompt = settings['analysis_prompt'] if settings['analysis_prompt'] else self.SYSTEM_PROMPT
            else:
                analysis_model = self.analysis_model
                analysis_model_name = config.GEMINI_ANALYSIS_MODEL
                system_prompt = self.SYSTEM_PROMPT

            logger.info(f"Analyzing defects with {analysis_model_name}...")
            analysis_prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ —Ñ–æ—Ç–æ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ù–∞–π–¥–∏ –≤—Å–µ –¥–µ—Ñ–µ–∫—Ç—ã." if context else "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ —Ñ–æ—Ç–æ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏."

            response = analysis_model.generate_content(
                [system_prompt, analysis_prompt, image],
                request_options={"timeout": 180}
            )

            logger.info(f"Defect analysis complete")
            result = json.loads(response.text)
            result['is_relevant'] = True  # Already checked by check_relevance

            logger.info(f"Analysis complete. Defects found: {len(result.get('items', []))}")

            return result

        except Exception as e:
            logger.error(f"Defect analysis error: {str(e)}", exc_info=True)
            return {
                "is_relevant": True,
                "items": [],
                "expert_summary": None,
                "error": str(e)
            }

    async def analyze_photo(self, photo_bytes: bytes, context: str = None) -> Dict[str, Any]:
        """
        Analyze construction photo for defects using two-stage approach:
        1. Fast model checks relevance
        2. Analysis model performs detailed defect analysis (if relevant)

        Args:
            photo_bytes: Photo binary data
            context: User-provided context (e.g., "–ñ–ö –ü–∏–æ–Ω–µ—Ä, 5 —ç—Ç–∞–∂")

        Returns:
            Dictionary with analysis results
        """
        try:
            logger.info(f"Starting two-stage photo analysis. Context: {context}, Photo size: {len(photo_bytes)} bytes")

            # Load image
            image = Image.open(io.BytesIO(photo_bytes))
            logger.info(f"Image loaded successfully. Size: {image.size}, Format: {image.format}")

            # Read settings from Google Docs (if configured)
            settings = None
            if config.GOOGLE_SETTINGS_DOC_ID:
                try:
                    logger.info(f"Reading AI settings from Google Doc: {config.GOOGLE_SETTINGS_DOC_ID}")
                    settings = settings_service.parse_ai_settings(config.GOOGLE_SETTINGS_DOC_ID)
                    logger.info(f"Settings loaded: relevance_model={settings['relevance_model']}, analysis_model={settings['analysis_model']}")
                except Exception as e:
                    logger.warning(f"Failed to read settings from Google Docs, using defaults: {e}")

            # Prepare models and prompts
            if settings:
                # Create models with settings from Google Docs
                relevance_model_name = settings['relevance_model']
                analysis_model_name = settings['analysis_model']

                relevance_model = genai.GenerativeModel(
                    relevance_model_name,
                    generation_config={
                        "temperature": 0.4,
                        "response_mime_type": "application/json"
                    }
                )

                analysis_model = genai.GenerativeModel(
                    analysis_model_name,
                    generation_config={
                        "temperature": 0.4,
                        "response_mime_type": "application/json"
                    }
                )

                # Use prompts from settings or defaults
                system_prompt_relevance = settings['relevance_prompt'] if settings['relevance_prompt'] else self.SYSTEM_PROMPT
                system_prompt_analysis = settings['analysis_prompt'] if settings['analysis_prompt'] else self.SYSTEM_PROMPT

                logger.info(f"Using custom models from settings: {relevance_model_name}, {analysis_model_name}")
            else:
                # Use default models and prompts
                relevance_model = self.fast_model
                analysis_model = self.analysis_model
                system_prompt_relevance = self.SYSTEM_PROMPT
                system_prompt_analysis = self.SYSTEM_PROMPT
                relevance_model_name = config.GEMINI_FAST_MODEL
                analysis_model_name = config.GEMINI_ANALYSIS_MODEL
                logger.info(f"Using default models: {relevance_model_name}, {analysis_model_name}")

            # STAGE 1: Quick relevance check with fast model
            logger.info(f"STAGE 1: Checking relevance with {relevance_model_name}...")
            relevance_prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–ü—Ä–æ–≤–µ—Ä—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Ñ–æ—Ç–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–º –æ–±—ä–µ–∫—Ç–æ–º. –ï—Å–ª–∏ —ç—Ç–æ –∫–æ—Ç, –µ–¥–∞, —Å–µ–ª—Ñ–∏ –∏–ª–∏ –Ω–µ —Å—Ç—Ä–æ–π–∫–∞ - –≤–µ—Ä–Ω–∏ is_relevant: false —Å —à—É—Ç–∫–æ–π. –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–π–∫–∞ - –≤–µ—Ä–Ω–∏ is_relevant: true." if context else "–ü—Ä–æ–≤–µ—Ä—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Ñ–æ—Ç–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–º –æ–±—ä–µ–∫—Ç–æ–º."

            relevance_response = relevance_model.generate_content(
                [system_prompt_relevance, relevance_prompt, image],
                request_options={"timeout": 120}  # 2 minutes timeout
            )

            logger.info(f"Relevance check complete. Response length: {len(relevance_response.text)} chars")
            logger.debug(f"Fast model response: {relevance_response.text}")

            relevance_result = json.loads(relevance_response.text)

            # If not relevant, return immediately
            if not relevance_result.get('is_relevant'):
                logger.info(f"Photo is NOT relevant. Stopping analysis.")
                return relevance_result

            # STAGE 2: Detailed analysis with analysis model
            logger.info(f"STAGE 2: Photo is relevant. Starting detailed analysis with {analysis_model_name}...")
            analysis_prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ —Ñ–æ—Ç–æ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ù–∞–π–¥–∏ –≤—Å–µ –¥–µ—Ñ–µ–∫—Ç—ã." if context else "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ —Ñ–æ—Ç–æ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏."

            analysis_response = analysis_model.generate_content(
                [system_prompt_analysis, analysis_prompt, image],
                request_options={"timeout": 180}  # 3 minutes timeout for detailed analysis
            )

            logger.info(f"Detailed analysis complete. Response length: {len(analysis_response.text)} chars")
            logger.debug(f"Analysis model response: {analysis_response.text}")

            # Parse response
            result = json.loads(analysis_response.text)

            logger.info(f"Analysis complete. Defects found: {len(result.get('items', []))}")

            return result

        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error in AI response: {str(e)}")
            return {
                "is_relevant": False,
                "joke": "‚ö†Ô∏è AI –≤–µ—Ä–Ω—É–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π –¥—Ä—É–≥–æ–µ —Ñ–æ—Ç–æ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.",
                "items": [],
                "expert_summary": None,
                "error": f"JSON parse error: {str(e)}"
            }
        except Exception as e:
            logger.error(f"AI service error: {str(e)}", exc_info=True)
            return {
                "is_relevant": False,
                "joke": "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≤—è–∑–∞—Ç—å—Å—è —Å AI-—Å–µ—Ä–≤–∏—Å–æ–º. –ü–æ–ø—Ä–æ–±—É–π —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.",
                "items": [],
                "expert_summary": None,
                "error": str(e)
            }

    def format_telegram_message(self, analysis: Dict[str, Any], context: str = None) -> str:
        """
        Format analysis results for Telegram message

        Args:
            analysis: Analysis results from AI
            context: Object context

        Returns:
            Formatted message string
        """
        if not analysis.get("is_relevant"):
            # Escape joke text as it may contain special characters
            joke_text = escape_markdown(analysis.get('joke', '–§–æ—Ç–æ –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤—É.'))
            return f"üòÑ {joke_text}"

        items = analysis.get("items", [])
        summary = analysis.get("expert_summary", "")

        # Escape context for safe Markdown
        safe_context = escape_markdown(context) if context else '–ù–µ —É–∫–∞–∑–∞–Ω'

        # Build message
        msg = f"üèó **–û–±—ä–µ–∫—Ç:** {safe_context}\n\n"
        msg += f"üö® **–í—ã—è–≤–ª–µ–Ω–æ –¥–µ—Ñ–µ–∫—Ç–æ–≤: {len(items)}**\n\n"

        # Add each defect
        for idx, item in enumerate(items, 1):
            criticality_emoji = {
                "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π": "üî•",
                "–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π": "‚ö†Ô∏è",
                "–ú–∞–ª–æ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π": "‚ÑπÔ∏è"
            }.get(item.get("criticality", ""), "‚ùì")

            # Escape all AI-generated text for safe Markdown
            defect_name = escape_markdown(item.get('defect', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–µ—Ñ–µ–∫—Ç'))
            location = escape_markdown(item.get('location', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'))
            norm = escape_markdown(item.get('norm', '–ù–æ—Ä–º–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞'))
            recommendation = escape_markdown(item.get('recommendation', '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω–∞'))
            criticality = escape_markdown(item.get('criticality', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'))

            msg += f"{idx}Ô∏è‚É£ **{defect_name}** ({criticality_emoji} {criticality})\n"
            msg += f"üìç *{location}*\n"
            msg += f"üìú *{norm}*\n"
            msg += f"üõ† {recommendation}\n\n"

        # Add summary (escape it too)
        if summary:
            safe_summary = escape_markdown(summary)
            msg += f"üìù **–ó–∞–∫–ª—é—á–µ–Ω–∏–µ:** {safe_summary}\n\n"

        msg += "‚úÖ **–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü—É!**"

        return msg


# Global AI service instance
ai_service = AIService()
